{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch as th\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.1 construce data\n",
    "구두점이 찍혀있는 512 이내의 텍스트를 입력으로 받아, 구두점이 없는 텍스트(sent)와 sent 내에서의 구두점이 위치해야 하는 위치 인덱스(label)의 리스트를 리턴"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def make_data(sentences: List[str]):\n",
    "    sents = []\n",
    "    labels = []\n",
    "    for sent in sentences:\n",
    "        sent = np.array(list(sent))\n",
    "        label = np.argwhere(sent=='.').flatten()\n",
    "        labels.append((label - np.arange(len(label))).tolist())\n",
    "        # sent[label] = ''\n",
    "        sent = np.delete(sent, label, axis=0)\n",
    "        sents.append(''.join(sent))\n",
    "\n",
    "    return sents, labels\n",
    "\n",
    "def validate_data(sentences, x, labels):\n",
    "    for i, (sent, label) in enumerate(zip(x, labels)):\n",
    "        sent_list = list(sent)\n",
    "        for idx in label[::-1]:\n",
    "            sent_list.insert(idx, '.')\n",
    "        if ''.join(sent_list) != sentences[i]:\n",
    "            raise ValueError(f\"{i}'th label has error\")\n",
    "    else:\n",
    "        print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "sentences = [\"you. cut the longer texts. off and only use the first 100 tokens. the original implementation truncates longer. sequences automatically.\",\n",
    "             \"now in my recently published paper there is a new method proposed called text guide. text guide is a text selection method that allows for improved performance when compared to naive or semi naive truncation methods.\"]\n",
    "x, labels = make_data(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you cut the longer texts off and only use the first 100 tokens the original implementation truncates longer sequences automatically', 'now in my recently published paper there is a new method proposed called text guide text guide is a text selection method that allows for improved performance when compared to naive or semi naive truncation methods']\n",
      "[[3, 24, 62, 107, 131], [83, 214]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "validate_data(sentences, x, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "# pretrained_model_name = \"klue/bert-base\"    # ko\n",
    "pretrained_model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "train_x = tokenizer(x, add_special_tokens=False, padding=True, truncation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "def binary_label(y_data, max_len):\n",
    "    b_data = []\n",
    "    for label in y_data:\n",
    "        binary = np.zeros(max_len, dtype=np.int32)\n",
    "        binary[label] = 1\n",
    "        b_data.append(binary)\n",
    "\n",
    "    b_data = np.array(b_data)\n",
    "    return b_data\n",
    "\n",
    "def construct_data_input(tokenizer, x, labels):\n",
    "    y_data = []\n",
    "    train_x = tokenizer(x, add_special_tokens=False, padding=True, truncation=True)\n",
    "\n",
    "    for i in tqdm(range(len(labels))):\n",
    "        label = []\n",
    "        for l in labels[i]:\n",
    "            label.append(train_x.char_to_token(i, l-1))\n",
    "        y_data.append(label)\n",
    "\n",
    "    X_data = [train_x['input_ids'], train_x['attention_mask']]\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = binary_label(y_data, len(train_x['input_ids'][0]))\n",
    "\n",
    "    return X_data, y_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11949.58it/s]\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = construct_data_input(tokenizer, x, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you textss longer automatically\n",
      "guide methods\n"
     ]
    }
   ],
   "source": [
    "for tokens, y in zip(X_data[0], y_data):\n",
    "    y = np.argwhere(y==1).flatten()\n",
    "    print(tokenizer.decode([tokens[l] for l in y]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Modeling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from keras import layers, models, initializers\n",
    "from transformers import TFBertModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "class TFBertForSentenceTokenizing(models.Model):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(TFBertForSentenceTokenizing, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(pretrained_model_name, from_pt=True)\n",
    "        self.outputs = layers.Dense(1,\n",
    "                                    kernel_initializer=initializers.initializers_v2.TruncatedNormal(mean=0.02),\n",
    "                                    activation='sigmoid',\n",
    "                                    name='outputs')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input_ids = inputs[0]\n",
    "        attention_mask = inputs[1]\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_output = bert_out[0]        # (bs, seq_len, h_dim)\n",
    "        logits = self.outputs(last_hidden_output)   # (bs, seq_len, 1)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSentenceTokenizing(pretrained_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1/1 [==============================] - 22s 22s/step - loss: 0.6735\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x219f9ea57c0>"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_data, y_data, epochs=1, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}