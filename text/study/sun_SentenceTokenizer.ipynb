{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple, Dict, Union, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- BERT token classification 으로 각 토큰이 eos가 되는지를 판단하는 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data\n",
    "\n",
    "- Data  \n",
    "    - https://www.kaggle.com/datasets/Cornell-University/arxiv 사용  \n",
    "    - 170만개의 article 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleaning strategy:\n",
    "        1. Remove all line-breaking characters.\n",
    "        2. Remove all content in '(...)' or '[...]'\n",
    "        3. Remove all LaTex formatting characters in '$...$'\n",
    "        4. Remove all Latex formatting characters without '$...$'\n",
    "        5. Remove all floating numbers\n",
    "        6. Remove all non-alphanumeric characters\n",
    "        7. Set punctuation to has no blank before and one blank after\n",
    "        8. Set all whitespace to single space\n",
    "    Args:\n",
    "        text: input text\n",
    "    Returns:\n",
    "        cleaned text\n",
    "    \"\"\"\n",
    "    regex = [(r'\\n+', ' '),\n",
    "           (r'\\([^\\(\\)]*\\)|\\[[^\\[\\]]*\\]', ' '),\n",
    "           (r'\\$.*\\$', ' '),\n",
    "           (r'\\\\[^\\s]+', ' '),\n",
    "           (r'\\d+\\.\\d+', ' '),\n",
    "           (r'[^a-zA-Z\\. ]', ' '),\n",
    "           (r' *\\. *', '. '),\n",
    "           (r' +', ' ')]\n",
    "    \n",
    "    cleaned = text\n",
    "    for pattern, repl in regex:\n",
    "        cleaned = re.sub(pattern, repl, cleaned)\n",
    "    \n",
    "    return cleaned.strip()\n",
    "\n",
    "def get_metadata(data_file='../data/arxiv-metadata-oai-snapshot.json'):\n",
    "    \"\"\"\n",
    "    For memory saving, generator will be returned\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "            \n",
    "def get_papers(n: int=100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n: number of papers to return\n",
    "    Returns:\n",
    "        list of papers\n",
    "    \"\"\"\n",
    "    metadata = get_metadata()\n",
    "    papers = []\n",
    "    success, fail = 0, 0\n",
    "    i = 0\n",
    "    for paper in tqdm(metadata):\n",
    "        paper_json = json.loads(paper)\n",
    "        if i==38:\n",
    "            print(f'=========== original ===========\\n{paper_json[\"abstract\"]}')\n",
    "        papers.append(clean(paper_json['abstract'].lower()))\n",
    "        if i==38:\n",
    "            print(f'=========== cleaned ===========\\n{papers[-1]}')\n",
    "        i += 1\n",
    "        if i == n:\n",
    "            break\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2536it [00:00, 12213.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== original ===========\n",
      "  The quadratic pion scalar radius, \\la r^2\\ra^\\pi_s, plays an important role\n",
      "for present precise determinations of \\pi\\pi scattering. Recently, Yndur\\'ain,\n",
      "using an Omn\\`es representation of the null isospin(I) non-strange pion scalar\n",
      "form factor, obtains \\la r^2\\ra^\\pi_s=0.75\\pm 0.07 fm^2. This value is larger\n",
      "than the one calculated by solving the corresponding Muskhelishvili-Omn\\`es\n",
      "equations, \\la r^2\\ra^\\pi_s=0.61\\pm 0.04 fm^2. A large discrepancy between both\n",
      "values, given the precision, then results. We reanalyze Yndur\\'ain's method and\n",
      "show that by imposing continuity of the resulting pion scalar form factor under\n",
      "tiny changes in the input \\pi\\pi phase shifts, a zero in the form factor for\n",
      "some S-wave I=0 T-matrices is then required. Once this is accounted for, the\n",
      "resulting value is \\la r^2\\ra_s^\\pi=0.65\\pm 0.05 fm^2. The main source of error\n",
      "in our determination is present experimental uncertainties in low energy S-wave\n",
      "I=0 \\pi\\pi phase shifts. Another important contribution to our error is the not\n",
      "yet settled asymptotic behaviour of the phase of the scalar form factor from\n",
      "QCD.\n",
      "\n",
      "=========== cleaned ===========\n",
      "the quadratic pion scalar radius r plays an important role for present precise determinations of scattering. recently yndur using an omn representation of the null isospin non strange pion scalar form factor obtains r fm. this value is larger than the one calculated by solving the corresponding muskhelishvili omn equations r fm. a large discrepancy between both values given the precision then results. we reanalyze yndur method and show that by imposing continuity of the resulting pion scalar form factor under tiny changes in the input phase shifts a zero in the form factor for some s wave i t matrices is then required. once this is accounted for the resulting value is r fm. the main source of error in our determination is present experimental uncertainties in low energy s wave i phase shifts. another important contribution to our error is the not yet settled asymptotic behaviour of the phase of the scalar form factor from qcd.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99999it [00:07, 13563.36it/s]\n"
     ]
    }
   ],
   "source": [
    "papers = get_papers(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. all next to leading order perturbative contributions from quark antiquark gluon quark and gluon gluon subprocesses are included as well as all orders resummation of initial state gluon radiation valid at next to next to leading logarithmic accuracy. the region of phase space is specified in which the calculation is most reliable. good agreement is demonstrated with data from the fermilab tevatron and predictions are made for more detailed tests with cdf and do data. predictions are shown for distributions of diphoton pairs produced at the energy of the large hadron collider. distributions of the diphoton pairs from the decay of a higgs boson are contrasted with those produced from qcd processes at the lhc showing that enhanced sensitivity to the signal can be obtained with judicious selection of events.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Data\n",
    "\n",
    "- Labels\n",
    "    - 해당 토큰이 eos가 되는지 여부\n",
    "\n",
    "ex)  \n",
    "Our hope is to em #power new use case #s. However, we are not sure how to do this yet.  \n",
    "0   0    0  0  -1 0      0   0   -1    1   0        0  0   0   0    0   0  0  0    1\n",
    "\n",
    "여기서 -1은 (편의를 위해 -100을 -1로 적음) 자동으로 무시되는 값 => subword 들 중 하나에만 유의미한 label을 부여하면 나머지는 무시해도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Features, ClassLabel\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets import features as ds_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hub:Dataset format으로 만들어주기 위해 dict로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = {'original': papers,\n",
    "          'source': [paper.replace('.', '') for paper in papers],\n",
    "          'is_eos': [[int(w[-1]=='.') for w in paper.split()] for paper in papers],\n",
    "          'id': list(range(len(papers)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`is_eos` : 토큰이 eos가 되는지를 판단하는 라벨  \n",
    "따라서, Dataset의 feature를 0, 1만 존재하는 `ClassLabel`로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = Dataset.from_dict(papers)\n",
    "papers.features['is_eos'] = ds_features.Sequence(ds_features.ClassLabel(2, names=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train: 50%, valid: 40%, test: 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = papers.train_test_split(test_size=0.5)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.2)\n",
    "\n",
    "papers = DatasetDict({'train': train_testvalid['train'],\n",
    "                        'valid': test_valid['train'],\n",
    "                        'test': test_valid['test']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distilbert는 같은 크기 대비 성능이 가장 좋은 BERT 모델입니다.  \n",
    "그리고 특정 token이 eos인지 아닌지 판별하는 문제를 풀고자 하므로 AutoModelforTokenClassifiaction을 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "pretrained_model_name = \"distilbert-base-uncased\"\n",
    "# pretrained_model_name = \"seg-model-distilBERT-finetuned\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이징을 거치면 여러 서브워드들이 생성되기 때문에, 각 단어마다 is_eos 태깅을 했던 것의 위치가 맞지 않게 됩니다.  \n",
    "따라서 이를 제대로 정렬하여 `labels` 로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import features as ds_features\n",
    "\n",
    "def tokenize_and_align_labels(papers: DatasetDict, label_pos: str='last') -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Since the labels are not aligned with the tokens due to subwords, this method aligns them.\n",
    "    \n",
    "    Args:\n",
    "        papers: DatasetDict of papers\n",
    "        label_pos: position of the is_eos label in the tokens\n",
    "    Returns:\n",
    "        DatasetDict of papers with aligned labels\n",
    "    \"\"\"\n",
    "    assert label_pos in ['all', 'first', 'last']\n",
    "    \n",
    "    tokenized_inputs = tokenizer(papers['source'], truncation=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(papers['is_eos']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        if label_pos == 'last':\n",
    "            word_ids = word_ids[::-1]\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_pos=='all' else -100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids[::-1] if label_pos=='last' else label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1996, 9575, 16014, 2015, 2008, 1037, 10713, 2275, 2004, 5097, 2057, 8980, 2195, 7680, 4031, 9872, 2015, 2525, 1999, 1996, 3906, 102], [101, 1996, 7613, 1997, 12702, 7770, 7741, 3463, 2875, 1996, 2312, 17454, 19036, 2594, 6112, 2145, 3464, 6801, 2317, 28984, 2031, 2042, 3818, 2000, 4863, 2122, 3463, 1998, 6516, 2000, 9002, 6022, 2000, 1996, 3742, 5166, 1997, 2256, 9088, 2174, 2195, 14679, 2006, 1996, 2535, 2209, 2011, 3180, 6351, 7722, 2317, 28984, 4839, 5294, 2860, 16584, 2063, 28984, 2024, 2245, 2000, 2022, 2081, 1997, 1037, 8150, 1997, 7722, 1998, 16231, 7978, 2135, 2037, 11520, 3446, 2003, 3469, 2084, 2216, 1997, 5171, 6351, 7722, 2317, 28984, 1998, 2027, 12985, 2000, 1999, 11365, 13464, 1999, 2460, 2335, 9289, 2229, 8821, 2027, 12346, 1037, 2204, 4018, 2005, 9990, 1996, 12702, 7770, 7741, 3463, 2182, 2057, 11628, 1999, 6987, 2023, 10744, 2011, 2478, 1996, 2087, 3522, 1998, 2039, 2000, 3058, 11520, 3162, 2005, 5294, 2317, 28984, 1998, 1037, 10125, 9758, 25837, 2029, 3138, 2046, 4070, 1996, 2087, 7882, 21375, 20407, 2057, 2424, 2008, 7722, 16231, 2317, 28984, 3685, 4070, 2005, 1037, 6937, 12884, 1997, 1996, 12702, 7770, 7741, 5995, 2875, 1996, 1048, 12458, 9174, 1997, 1996, 4233, 3988, 3742, 3853, 2348, 2070, 12702, 7770, 7741, 2824, 2071, 2022, 2349, 2000, 7722, 16231, 2317, 28984, 1996, 2317, 11229, 2313, 16605, 2012, 2087, 1037, 2000, 1996, 3742, 1997, 1996, 21375, 17201, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, -100, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 1, -100], [-100, 0, 0, 0, -100, -100, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, -100, -100, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(papers['train'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DatasetDict` 객체에 mapping을 통해 method를 적용시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_and_align_labels at 0x7f5030197160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d98fc8213444e90ad0e645117b9b556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a34ad0279a4d77bc53e8c66fc58bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46765848590e45d3beacf1054a97b3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = papers.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에는 BERT가 받아들이는 parameter: intput_ids, attention_mask, labels 만이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['original', 'source', 'id', 'is_eos'])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'].features['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "\n",
    "data batch를 만들어주는 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 10:05:30.134528: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER 문제에 쓰이는 seqeval metric을 eos를 분류하는 binary cls에 맞도록 강제로 바꿔봤습니다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[['O', 'B-PER']], references=[['B-PER', 'O']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_list = ['O', 'B-PER']\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p: predictions and labels, which are BERT prediction output\n",
    "    Returns:\n",
    "        metrics: dictionary of metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer API를 사용하여 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 16\n",
    "model_name = pretrained_model_name.split(\"/\")[-1]\n",
    "args = TrainingArguments(f\"{model_name}-finetuned\",\n",
    "                         evaluation_strategy='epoch',\n",
    "                         learning_rate=2e-5,\n",
    "                         per_device_train_batch_size=batch_size,\n",
    "                         per_device_eval_batch_size=batch_size,\n",
    "                         num_train_epochs=3,\n",
    "                         weight_decay=0.01,\n",
    "                         push_to_hub=False)\n",
    "\n",
    "trainer = Trainer(model, args,\n",
    "                  train_dataset=tokenized_datasets['train'],\n",
    "                  eval_dataset=tokenized_datasets['valid'],\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 50000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2346\n",
      "/home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/2346 17:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.030014</td>\n",
       "      <td>0.888854</td>\n",
       "      <td>0.864517</td>\n",
       "      <td>0.876516</td>\n",
       "      <td>0.989262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.027655</td>\n",
       "      <td>0.891486</td>\n",
       "      <td>0.885924</td>\n",
       "      <td>0.888697</td>\n",
       "      <td>0.990217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.026981</td>\n",
       "      <td>0.890030</td>\n",
       "      <td>0.893360</td>\n",
       "      <td>0.891692</td>\n",
       "      <td>0.990433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbert-base-uncased-finetuned/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned/checkpoint-500/special_tokens_map.json\n",
      "/home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned/checkpoint-1000/special_tokens_map.json\n",
      "/home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned/checkpoint-1500/special_tokens_map.json\n",
      "/home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned/checkpoint-2000/special_tokens_map.json\n",
      "/home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2346, training_loss=0.034029456558942996, metrics={'train_runtime': 1067.9166, 'train_samples_per_second': 140.46, 'train_steps_per_second': 2.197, 'total_flos': 1.220557177589568e+16, 'train_loss': 0.034029456558942996, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='638' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 40:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.026981081813573837,\n",
       " 'eval_precision': 0.8900300888088372,\n",
       " 'eval_recall': 0.8933598771044299,\n",
       " 'eval_f1': 0.8916918744132679,\n",
       " 'eval_accuracy': 0.9904326962501993,\n",
       " 'eval_runtime': 138.0474,\n",
       " 'eval_samples_per_second': 289.756,\n",
       " 'eval_steps_per_second': 4.527,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to seg-model-distilBERT-finetuned\n",
      "Configuration saved in seg-model-distilBERT-finetuned/config.json\n",
      "Model weights saved in seg-model-distilBERT-finetuned/pytorch_model.bin\n",
      "tokenizer config file saved in seg-model-distilBERT-finetuned/tokenizer_config.json\n",
      "Special tokens file saved in seg-model-distilBERT-finetuned/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('seg-model-distilBERT-finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [\n",
    "    \"\"\"USB was designed to standardize the connection of peripherals to personal computers, both to communicate with and to supply electric power. It has largely replaced interfaces such as serial ports and parallel ports, and has become commonplace on a wide range of devices. Examples of peripherals that are connected via USB include computer keyboards and mice, video cameras, printers, portable media players, mobile (portable) digital telephones, disk drives, and network adapters. USB connectors have been increasingly replacing other types as charging cables of portable devices.\"\"\",\n",
    "    \"\"\"Historical linguistics is the study of language changes in history, particularly with regard to a specific language or a group of languages. Western trends in historical linguistics date back to roughly the late 18th century, when the discipline grew out of philology, the study of ancient texts and oral traditions.[15] Historical linguistics emerged as one of the first few sub-disciplines in the field, and was most widely practiced during the late 19th century.[16] Despite a shift in focus in the twentieth century towards formalism and generative grammar, which studies the universal properties of language, historical research today still remains a significant field of linguistic inquiry. Subfields of the discipline include language change and grammaticalisation.[17]\"\"\",\n",
    "    \"\"\"in the formation education that is carried out within the scope of undergraduate and non thesis graduate programs within the same university different criteria are used to evaluate students success. in this study classification accuracy of letter grades that are generated to evaluate students success using relative and absolute criteria and decisions for students passing or failing a course were examined. within the scope of this study it was also intended to determine the cut off point required for students to pass a course. in this regard midterm and final grades of a total of students. first correct classification percentages of the letter grades that the students scored with absolute and relative evaluations were calculated. then classification percentages for decisions regarding passing or failing a course were examined.\"\"\",\n",
    "    \"\"\"Hi. I'm Phil from BBC learning English. Today, I'm going to tell you how to use make and do not. I can be tricky and there are some exceptions, but Hero full things to remember use make when we create something.  Play Miss Kate. We use due to talk about an activity. What are you doing? We can use mic to talk about something that causes a reaction. This music reading makes me want to sing. We can also use do with General activities. What are you doing, tomorrow? I will doing anything.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = load_metric('precision')\n",
    "recall = load_metric('recall')\n",
    "f1 = load_metric('f1')\n",
    "accuracy = load_metric('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4dd59a0f4548d29a39fe40419b16b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 4\n",
      "  Batch size = 64\n",
      "/home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b684cde6a248478fbe54d09ac8d73f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.9786729857819905,\n",
      " 'test_f1': 0.8085106382978724,\n",
      " 'test_loss': 0.05699896812438965,\n",
      " 'test_precision': 0.8636363636363636,\n",
      " 'test_recall': 0.76,\n",
      " 'test_runtime': 6.3409,\n",
      " 'test_samples_per_second': 0.631,\n",
      " 'test_steps_per_second': 0.158}\n",
      "\n",
      "========= text [0] =========\n",
      "accuracy: 100.00\n",
      "----- original text -----\n",
      "usb was designed to standardize the connection of peripherals to personal computers both to communicate with and to supply electric power.\n",
      " it has largely replaced interfaces such as serial ports and parallel ports and has become commonplace on a wide range of devices.\n",
      " examples of peripherals that are connected via usb include computer keyboards and mice video cameras printers portable media players mobile digital telephones disk drives and network adapters.\n",
      " usb connectors have been increasingly replacing other types as charging cables of portable devices.\n",
      "\n",
      "----- predicted text -----\n",
      "usb was designed to standardize the connection of peripherals to personal computers both to communicate with and to supply electric power.\n",
      " it has largely replaced interfaces such as serial ports and parallel ports and has become commonplace on a wide range of devices.\n",
      " examples of peripherals that are connected via usb include computer keyboards and mice video cameras printers portable media players mobile digital telephones disk drives and network adapters.\n",
      " usb connectors have been increasingly replacing other types as charging cables of portable devices.\n",
      "\n",
      "========= text [1] =========\n",
      "accuracy: 100.00\n",
      "----- original text -----\n",
      "historical linguistics is the study of language changes in history particularly with regard to a specific language or a group of languages.\n",
      " western trends in historical linguistics date back to roughly the late th century when the discipline grew out of philology the study of ancient texts and oral traditions.\n",
      " historical linguistics emerged as one of the first few sub disciplines in the field and was most widely practiced during the late th century.\n",
      " despite a shift in focus in the twentieth century towards formalism and generative grammar which studies the universal properties of language historical research today still remains a significant field of linguistic inquiry.\n",
      " subfields of the discipline include language change and grammaticalisation.\n",
      "\n",
      "----- predicted text -----\n",
      "historical linguistics is the study of language changes in history particularly with regard to a specific language or a group of languages.\n",
      " western trends in historical linguistics date back to roughly the late th century when the discipline grew out of philology the study of ancient texts and oral traditions.\n",
      " historical linguistics emerged as one of the first few sub disciplines in the field and was most widely practiced during the late th century.\n",
      " despite a shift in focus in the twentieth century towards formalism and generative grammar which studies the universal properties of language historical research today still remains a significant field of linguistic inquiry.\n",
      " subfields of the discipline include language change and grammaticalisation.\n",
      "\n",
      "========= text [2] =========\n",
      "accuracy: 98.52\n",
      "----- original text -----\n",
      "in the formation education that is carried out within the scope of undergraduate and non thesis graduate programs within the same university different criteria are used to evaluate students success.\n",
      " in this study classification accuracy of letter grades that are generated to evaluate students success using relative and absolute criteria and decisions for students passing or failing a course were examined.\n",
      " within the scope of this study it was also intended to determine the cut off point required for students to pass a course.\n",
      " in this regard midterm and final grades of a total of students.\n",
      " first correct classification percentages of the letter grades that the students scored with absolute and relative evaluations were calculated.\n",
      " then classification percentages for decisions regarding passing or failing a course were examined.\n",
      "\n",
      "----- predicted text -----\n",
      "in the formation education that is carried out within the scope of undergraduate and non thesis graduate programs within the same university different criteria are used to evaluate students success.\n",
      " in this study classification accuracy of letter grades that are generated to evaluate students success using relative and absolute criteria and decisions for students passing or failing a course were examined.\n",
      " within the scope of this study.\n",
      " it was also intended to determine the cut off point required for students to pass a course.\n",
      " in this regard midterm and final grades of a total of students first correct classification percentages of the letter grades that the students scored with absolute and relative evaluations were calculated.\n",
      " then classification percentages for decisions regarding passing or failing a course were examined.\n",
      "\n",
      "========= text [3] =========\n",
      "accuracy: 92.78\n",
      "----- original text -----\n",
      "hi.\n",
      " i m phil from bbc learning english.\n",
      " today i m going to tell you how to use make and do not.\n",
      " i can be tricky and there are some exceptions but hero full things to remember use make when we create something.\n",
      " play miss kate.\n",
      " we use due to talk about an activity.\n",
      " what are you doing we can use mic to talk about something that causes a reaction.\n",
      " this music reading makes me want to sing.\n",
      " we can also use do with general activities.\n",
      " what are you doing tomorrow i will doing anything.\n",
      "\n",
      "----- predicted text -----\n",
      "hi i.\n",
      " m.\n",
      " phil from bbc learning english today i m going to tell you how to use make and do not.\n",
      " i can be tricky and there are some exceptions but hero full things to remember use make when we create something play miss kate we use due to talk about an activity.\n",
      " what are you doing we can use mic to talk about something that causes a reaction.\n",
      " this music reading makes me want to sing.\n",
      " we can also use do with general activities what are you doing tomorrow i will doing anything.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['usb was designed to standardize the connection of peripherals to personal computers both to communicate with and to supply electric power. it has largely replaced interfaces such as serial ports and parallel ports and has become commonplace on a wide range of devices. examples of peripherals that are connected via usb include computer keyboards and mice video cameras printers portable media players mobile digital telephones disk drives and network adapters. usb connectors have been increasingly replacing other types as charging cables of portable devices.',\n",
       " 'historical linguistics is the study of language changes in history particularly with regard to a specific language or a group of languages. western trends in historical linguistics date back to roughly the late th century when the discipline grew out of philology the study of ancient texts and oral traditions. historical linguistics emerged as one of the first few sub disciplines in the field and was most widely practiced during the late th century. despite a shift in focus in the twentieth century towards formalism and generative grammar which studies the universal properties of language historical research today still remains a significant field of linguistic inquiry. subfields of the discipline include language change and grammaticalisation.',\n",
       " 'in the formation education that is carried out within the scope of undergraduate and non thesis graduate programs within the same university different criteria are used to evaluate students success. in this study classification accuracy of letter grades that are generated to evaluate students success using relative and absolute criteria and decisions for students passing or failing a course were examined. within the scope of this study. it was also intended to determine the cut off point required for students to pass a course. in this regard midterm and final grades of a total of students first correct classification percentages of the letter grades that the students scored with absolute and relative evaluations were calculated. then classification percentages for decisions regarding passing or failing a course were examined.',\n",
       " 'hi i. m. phil from bbc learning english today i m going to tell you how to use make and do not. i can be tricky and there are some exceptions but hero full things to remember use make when we create something play miss kate we use due to talk about an activity. what are you doing we can use mic to talk about something that causes a reaction. this music reading makes me want to sing. we can also use do with general activities what are you doing tomorrow i will doing anything.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def demo(texts: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        texts: list of texts to demonstrate.\n",
    "    Returns:\n",
    "        punctated texts by BERT prediction.\n",
    "    \"\"\"\n",
    "    if not isinstance(texts, list):\n",
    "        texts = [texts]\n",
    "    \n",
    "    clean_texts = [clean(text).lower().strip() for text in texts]\n",
    "    text_dataset = {'original': clean_texts,\n",
    "                    'source': [text.replace('.', '') for text in clean_texts],\n",
    "                    'is_eos': [[int(w[-1]=='.') for w in text.split()] for text in clean_texts]}\n",
    "    \n",
    "    text_dataset = Dataset.from_dict(text_dataset)\n",
    "    text_dataset.features['is_eos'] = ds_features.Sequence(ds_features.ClassLabel(2, names=[0,1]))\n",
    "    \n",
    "    samples = DatasetDict({'sample': text_dataset})\n",
    "    samples = samples.map(tokenize_and_align_labels, batched=True)\n",
    "    samples = samples.remove_columns(['original', 'source', 'is_eos'])['sample']\n",
    "    \n",
    "    preds, labels, infos = trainer.predict(samples)\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    labels = np.clip(labels, 0, np.inf).astype(int)\n",
    "    pprint(infos)\n",
    "    print()\n",
    "    \n",
    "    pred_texts = []\n",
    "    \n",
    "    for i, (pred, label) in enumerate(zip(preds, labels)):\n",
    "        print(f'========= text [{i}] =========')\n",
    "        seqlen = len(samples['input_ids'][i])\n",
    "\n",
    "        print(f'accuracy: {np.mean((pred==label)[:seqlen])*100:.2f}')\n",
    "             \n",
    "        print(f'----- original text -----')\n",
    "        print(text_dataset['original'][i].replace('.', '.\\n'))\n",
    "        \n",
    "        pred_text = tokenizer.convert_ids_to_tokens(samples['input_ids'][i])\n",
    "        pred_text = [w if l==0 else w+'.' for w,l in zip(pred_text, pred)]\n",
    "        pred_text = ' '.join(pred_text[1:-1]).replace(' ##', '')\n",
    "        pred_texts.append(pred_text)\n",
    "        print(f'----- predicted text -----')\n",
    "        print(pred_text.replace('.', '.\\n'))\n",
    "    \n",
    "    return pred_texts\n",
    "    \n",
    "demo(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "`punctuate()` 메소드만 쓰면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 12:23:20.638473: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple, Dict, List, Any\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    DataCollatorForTokenClassification\n",
    "    )\n",
    "\n",
    "def load_sent_seg_model(model_name: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "    return tokenizer, model, data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'seg-model-distilBERT-finetuned'\n",
    "# tokenizer, model, data_collator = load_sent_seg_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch as th\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "# model, tokenizer, data collator needed!\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    regex = [(r'\\n+', ' '),\n",
    "           (r'\\([^\\(\\)]*\\)|\\[[^\\[\\]]*\\]', ' '),\n",
    "           (r'\\$.*\\$', ' '),\n",
    "           (r'\\\\[^\\s]+', ' '),\n",
    "           (r'\\d+\\.\\d+', ' '),\n",
    "           (r'[^a-zA-Z\\. ]', ' '),\n",
    "           (r' *\\. *', '. '),\n",
    "           (r' +', ' ')]\n",
    "    \n",
    "    cleaned = text\n",
    "    for pattern, repl in regex:\n",
    "        cleaned = re.sub(pattern, repl, cleaned)\n",
    "    \n",
    "    return cleaned.strip()\n",
    "\n",
    "def construct_dataset(tokenizer, texts: List[str], puncuated=True):\n",
    "    clean_texts = [clean(text).lower().strip() for text in texts]\n",
    "    if puncuated:\n",
    "        dataset = Dataset.from_dict({'original': clean_texts,\n",
    "                                     'source': [text.replace('.', '') for text in clean_texts],\n",
    "                                     'is_eos': [[int(w[-1]=='.') for w in text.split()] for text in clean_texts]})\n",
    "\n",
    "        dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "        dataset = dataset.remove_columns(['original', 'source', 'is_eos'])\n",
    "    \n",
    "    else:\n",
    "        dataset = Dataset.from_dict(tokenizer(clean_texts, truncation=True))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def punctuate(texts: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        texts: STT완료한 non-puncuated 문장들, 꼭 List[str] 형식으로 넣어줄 것.\n",
    "    Returns:\n",
    "        주어진 texts들에 대해 온점을 찍은 문장들\n",
    "    \"\"\"\n",
    "    tokenizer, model, data_collator = load_sent_seg_model(model_name)\n",
    "    dataset = construct_dataset(tokenizer, texts, puncuated=False)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=1, \n",
    "                            shuffle=False, \n",
    "                            collate_fn=data_collator)\n",
    "    \n",
    "    def insert_punct(token_ids, preds):\n",
    "        args = th.argwhere(preds==1)\n",
    "        subwords = tokenizer.convert_ids_to_tokens(token_ids.squeeze())\n",
    "        for arg in args:\n",
    "            subwords[arg] += '.'\n",
    "        return subwords\n",
    "            \n",
    "    pred_texts = []\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        pred = th.argmax(logits, dim=2).squeeze()\n",
    "        \n",
    "        pred_text = insert_punct(input_ids, pred)\n",
    "        pred_text = ' '.join(pred_text[1:-1]).replace(' ##', '')\n",
    "        pred_texts.append(pred_text)\n",
    "    \n",
    "    return pred_texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = punctuate([text.replace('.', ' ') for text in sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== doc 0 =====\n",
      "usb was designed to standardize the connection of peripherals to personal computers both to communicate with and to supply electric power.\n",
      " it has largely replaced interfaces such as serial ports and parallel ports and has become commonplace on a wide range of devices.\n",
      " examples of peripherals that are connected via usb include computer keyboards and mice video cameras printers portable media players mobile digital telephones disk drives and network adapters.\n",
      " usb connectors have been increasingly replacing other types as charging cables of portable devices.\n",
      "\n",
      "===== doc 1 =====\n",
      "historical linguistics is the study of language changes in history particularly with regard to a specific language or a group of languages.\n",
      " western trends in historical linguistics date back to roughly the late th century when the discipline grew out of philology the study of ancient texts and oral traditions.\n",
      " historical linguistics emerged as one of the first few sub disciplines in the field and was most widely practiced during the late th century.\n",
      " despite a shift in focus in the twentieth century towards formalism and generative grammar which studies the universal properties of language historical research today still remains a significant field of linguistic inquiry.\n",
      " subfields of the discipline include language change and grammaticalisation.\n",
      "\n",
      "===== doc 2 =====\n",
      "in the formation education that is carried out within the scope of undergraduate and non thesis graduate programs within the same university different criteria are used to evaluate students success.\n",
      " in this study classification accuracy of letter grades that are generated to evaluate students success using relative and absolute criteria and decisions for students passing or failing a course were examined.\n",
      " within the scope of this study.\n",
      " it was also intended to determine the cut off point required for students to pass a course.\n",
      " in this regard midterm and final grades of a total of students first correct classification percentages of the letter grades that the students scored with absolute and relative evaluations were calculated.\n",
      " then classification percentages for decisions regarding passing or failing a course were examined.\n",
      "\n",
      "===== doc 3 =====\n",
      "hi i.\n",
      " m.\n",
      " phil from bbc learning english today i m going to tell you how to use make and do not.\n",
      " i can be tricky and there are some exceptions but hero full things to remember use make when we create something play miss kate we use due to talk about an activity.\n",
      " what are you doing we can use mic to talk about something that causes a reaction.\n",
      " this music reading makes me want to sing.\n",
      " we can also use do with general activities what are you doing tomorrow i will doing anything.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(result):\n",
    "    print(f'===== doc {i} =====')\n",
    "    print(res.replace('.', '.\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17b00b123a161cbd9e3962c7a92be75ccd1912f9e6a75183c44e7ca10728623d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
